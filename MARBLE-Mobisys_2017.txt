===========================================================================
                       ACM MobiSys 2017 Review #75A
---------------------------------------------------------------------------
       Paper #75: MARBLE: Mobile Augmented Reality Using BLE Beacons
---------------------------------------------------------------------------

                      Overall merit: 3. Weak accept
                 Reviewer expertise: 3. Knowledgeable
Is this paper exciting or thought-provoking?:
                                     2. Somewhat, there were some
                                        interesting aspects.

                         ===== Paper summary =====


The idea of marble is to use BLE beacons to transmit location-aware augmentations of a physical space to nearby mobile users.  For example, a user near a marble beacon would see  an animation of an avatar in front of the coffee machine when visiting the coffee shop.  The idea is to use computer vision, BLE RSS and IMUs to figure out where the user is in physical space.

                      ===== Principal strengths =====


- cute idea: Marble challenges conventional assumptions about how to do AR.

                     ===== Principal weaknesses =====


- not clear that BLE localization is sufficient granularity for good results
- it is not clear why DC power consumption is a concern
- it is not clear storage requirement analysis is durable/long lasting
- CV feature matching approach doesn’t work well for spaces that are highly dynamic (probably most public outdoor scenes)
- keypoints of head and hands: it is a bit strange to only render those?  Why not just render entire avatar? Do you render “avatar” in a depth correct way?

                      ===== Comments for author =====


Most AR products (such as HoloLens) are focusing on inside-out tracking which is quickly becoming the industry norm. The authors essentially propose outside-in tracking, which is on the one hand, a bit odd, but on the other hand, fresh as well.  The version the authors present is rough in terms of its execution matching its vision.  However, there is an interesting vision there that is worth exploring.

I was at first surprised that the authors seriously proposed using Bluetooth RSS as a way to perform localization.  As expected, it appears that the contribution of RSS is not substantial.  Therefore, wouldn’t it simplify the system design to just cut RSS and just focus on cv-based localization?  The authors’ eventual system looks very similar to a CV-based scene reconstruction effort.

The authors have not sufficiently covered extensive related work in Computer Vision-based localization.  Photosynth comes to mind as a notable piece of work, though there are many others as well.

Minor questions.
- why does BLE only give x,y and not z?  with enough BLE beacons, it is possible to obtain z as well
- 7c is much less compelling than 7a. why dumb it down?

It is also interesting for one to entertain a much less convoluted scheme whereby one mobile recorder captures the scene, and then it plays back on someone else’s phone.  The downside is that it can only playback from the recorded point of view.  However, it relieves many of the BLE infrastructure setup issues.

===========================================================================
                       ACM MobiSys 2017 Review #75B
---------------------------------------------------------------------------
       Paper #75: MARBLE: Mobile Augmented Reality Using BLE Beacons
---------------------------------------------------------------------------

                      Overall merit: 3. Weak accept
                 Reviewer expertise: 3. Knowledgeable
Is this paper exciting or thought-provoking?:
                                     2. Somewhat, there were some
                                        interesting aspects.

                         ===== Paper summary =====

The paper proposes a system called MARBLE, which utilizes BLE beacons to realize real-time mobile augmented reality. The system composes of a set of low-cost BLE beacons and cameras deployed in different positions around an area. In the capture and store phase, a video is taken while a person is moving in the area covered by the cameras. The motion of several key points of the person (head and two hands) are detected, tracked and stored in beacons. The stored motions combined with visual features of each camera is broadcasted over the connectionless BLE packets. During the read and render phase, a mobile device with a camera compute its direction and position according to the IMU data, camera data, and received data. And then the person is rendered in the camera view at the right position.

                      ===== Principal strengths =====

- MARBLE proposes an interesting application.

- The system systematically fuses IMU, camera and bluetooth signals to estimate the position and pose of the mobile device.

- Real-time system with short delay.

                     ===== Principal weaknesses =====

- Multiple devices need to deployed.

- The motivation of using BLE is not convincing.

- No evaluation on energy.

                      ===== Comments for author =====

The overall system is a cute idea. I also appreciate that the authors design an algorithm to systematically fuse camera data, IMU, and BLE data to achieve high accuracy in location tracking and pose estimation.

In BLE-based location estimation, why is z component not included in the formula?

I am not fully convinced by the motivation of using BLE since the low data rate is a major constraint of the system. Energy is used as a main argument for using BLE. However, image capturing and computation seem to be the dominant energy consumption in MARBLE.

Given that low energy is claimed to be a key advantage, it will be very helpful if the authors can demonstrate the energy consumption of the whole system as well as that of each major components.

===========================================================================
                       ACM MobiSys 2017 Review #75C
---------------------------------------------------------------------------
       Paper #75: MARBLE: Mobile Augmented Reality Using BLE Beacons
---------------------------------------------------------------------------

                      Overall merit: 3. Weak accept
                 Reviewer expertise: 2. Some familiarity
Is this paper exciting or thought-provoking?:
                                     2. Somewhat, there were some
                                        interesting aspects.

                         ===== Paper summary =====

This paper presents a mobile AR system that uses BLE beacons to store object visual information and provide user localization.  The key contribution is a set of mechanisms to identify necessary information to store on BLE  devices to enable real-time AR rendering, while taking into account the user location and pose.

                      ===== Principal strengths =====

+ interesting and ambitious problem;
+ well written, provide a nice tutorial on AR and its interaction with BLE broadcasts;
+ nice system design;

                     ===== Principal weaknesses =====

- the key contribution is mostly visual feature section;

- has not considered the reliability of BLE devices.

                      ===== Comments for author =====

This is an interesting system effort that enables AR on cheap (but many) BLE devices. The key contribution is the design and selection of visual features to be stored on BLE devices and then broadcasted to users to help 3D object rendering.  This seems to be the first AR system that uses solely BLE devices.

The whole paper focuses on showing the feasibility of such system, but the study seems to be premature and fail to address several practical issues. For example:

- It is unclear whether the experimented scenarios are representative, and whether the amount of visual features will increase drastically as the environment and dynamics become more complex.

- What happens when some of the BLE devices fail, either due to hardware/software failure or external interference?

- Does the BLE device density/placement affect the performance?

- User location and pose estimation seem to be important factors on the user performance; while the study shows their individual error performance, but how do these errors affect user experience?

===========================================================================
                       ACM MobiSys 2017 Review #75D
---------------------------------------------------------------------------
       Paper #75: MARBLE: Mobile Augmented Reality Using BLE Beacons
---------------------------------------------------------------------------

                      Overall merit: 2. Weak reject
                 Reviewer expertise: 3. Knowledgeable
Is this paper exciting or thought-provoking?:
                                     2. Somewhat, there were some
                                        interesting aspects.

                         ===== Paper summary =====

_MARBLE_ is: eight Raspberry PI-augmented Bluetooth beacons connected
to a stereo camera and a Nexus 5 smartphone.  The system operates in
two phases, first capturing and storing information, then reading and
rendering an augmented reality stream.

The bulk of the paper is dedicated to vision algorithms (for which
there is considerable background work) for estimating location of
people and objects in a scene.  The system also assumes that the user
is holding a device, from which it gains inertial measurements.

An experimental evaluation tests an application where a person's
gestures and poses are captured, and then a virtual object avatar
rendered in the empty environment _at a later time_.  Experiments
measure CPU and memory usage on the smartphone, run time of various
components of the processing, and position error, concluding that
cameras are significantly better than Bluetooth for localizing things,
inertial measurements are slightly better, and sampling at a framerate
of 12-15 Hz suffices for capturing motion.

                     ===== Principal weaknesses =====

- Why can't this all happen on the smartphone using the smartphone's
  camera?  Or with a Microsoft Hololens?  Or, with just a bunch of
  cameras in a room?  It's not clear what the Bluetooth beacons are
  actually adding to the system design besides a location service.

- Writing issues obscure the technical contributions of this paper
  (please see detailed comments below).

- As best this reviewer can understand, the system does not work in
  real-time, instead operating in two distinct phases.

                      ===== Comments for author =====

It would help reading the abstract if you clarified what a "marker"
is.

Who are you quoting, in the second paragraph of the introduction?
Also in the introduction, you assert that "creating the first mobile
augmented reality infrastructure" is seeemingly impossible, but I'm
not sure (at this stage in my reading of your paper) what that
actually means or entails, or enables.  How is this enabling any
applications further than, e.g. those that Microsoft HoloLens enables?

Section 2.2 changes the design, assuming that a camera is attached to
each Bluetooth beacon.  Best to be consistent across the entire paper.

Section 4.2 reveals that the system is also comprised of a smartphone
held in the hand of the user.  Again, better to state this up front so
the reader has a clear idea of *what* exactly your system is.

Figure 7, the _key_ figure that showcases your proposed application,
is never discussed in the text -- this makes it difficult for the
reader to understand the most important (I think!) motivating
application for your system.

===========================================================================
                       ACM MobiSys 2017 Review #75E
---------------------------------------------------------------------------
       Paper #75: MARBLE: Mobile Augmented Reality Using BLE Beacons
---------------------------------------------------------------------------

                      Overall merit: 2. Weak reject
                 Reviewer expertise: 2. Some familiarity
Is this paper exciting or thought-provoking?:
                                     1. No, nothing exciting in this paper.

                         ===== Paper summary =====

This paper proposes to use BLE beacons to facilitate indoor augmented reality. The authors consider the motion capture scenario, with the goal of replaying movements of a 3D object. The idea is to use a set of cameras to capture a 3D object data and transmit key features of a 3D object as BLE beacons to the viewer's mobile device. The viewer's device can later render the object considering the camera's orientation and position.

                      ===== Principal strengths =====

+ the overall systems design seems solid
+ the design is demonstrated via real prototype implementation

                     ===== Principal weaknesses =====

- the role played by BLE in the current framework does not make a strong case for the need of BLE and alternative methods seem to also work fine
- the capability of the current system (only rendering a 3D object as set of rectangles, 0.6 second to render a scene) seems insufficient and preliminary for the motion caption application

                      ===== Comments for author =====

Mobile augmented reality is an interesting topic. I appreciate the authors' efforts along the line. However, I do not see the necessity of using BLE beacons for enabling augmented reality. As the paper is set out to render a virtual 3D object (motion capture scenario), the fine details of the object seem important to me to deliver good user experiences. With the limited data rate, BLE does not appear to be a good candidate here. The current system renders a 3D object as a set of rectangles (Fig. 7c) and needs 0.6 second to render a scene (i.e., < 2 FPS). It is unclear to me what types of augmented reality applications can be supported. It will help if the authors can clarify the motivation. Perhaps mmWave is a better fit for delivering a large volume of data and enabling high rendering rate?

Perhaps the authors can focus on adding text labels to existing physical objects, as a type of augmented reality application? In this case, the required data rate is low and BLE can be more appropriate given its low cost and low power.

If I understand the system correctly, in the current framework, BLE is used to transmit features of the 3D object and also to locate the 2D location of the viewer. Would IR transceivers also do the job? It occurs to me that cameras play a more important role here, as the generation of visual feature and capturing of 3D object movement rely on the set of cameras set up in the environment, while the estimation of viewer location use the camera of viewer's phone. BLE seems to play a secondary role and can be replaced by other alternatives.

minor things:

-- Section 2.3: 10 x 10 dimensions --> what's the unit here?

-- Section 7: I -> In
